{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iBMcobPHdD8O"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqOt6Sv7AsMi"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3PM6GVHcC31"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 160 # All images will be resized to 160x160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yic-I66m6Isv"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for melanoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "IMAGE_WIDTH=160\n",
    "IMAGE_HEIGHT=160\n",
    "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "\n",
    "# red, green and blue channels\n",
    "IMAGE_CHANNELS=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_name', 'patient_id', 'sex', 'age_approx',\n",
       "       'anatom_site_general_challenge', 'diagnosis', 'benign_malignant',\n",
       "       'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"../input/siim-isic-melanoma-classification/train.csv\")\n",
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33126, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe=dataframe.drop(labels=[\"patient_id\",\"sex\",\"age_approx\",\"anatom_site_general_challenge\",\"diagnosis\",\"benign_malignant\"],axis=1)\n",
    "dataframe[\"image_name\"]=dataframe[\"image_name\"].apply(lambda x:x+\".jpg\")\n",
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_df=dataframe[dataframe.target==0].copy()\n",
    "malignant_df = dataframe[dataframe.target==1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_2637011.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015719.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0052212.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_name  target\n",
       "0  ISIC_2637011.jpg       0\n",
       "1  ISIC_0015719.jpg       0\n",
       "2  ISIC_0052212.jpg       0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benign_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>ISIC_0149568.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>ISIC_0188432.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>ISIC_0207268.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image_name  target\n",
       "91   ISIC_0149568.jpg       1\n",
       "235  ISIC_0188432.jpg       1\n",
       "314  ISIC_0207268.jpg       1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malignant_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26033\n",
      "6509\n",
      "467\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "train_benign,val_benign = train_test_split(benign_df,test_size=0.2,random_state=0)\n",
    "train_malignant, val_malignant = train_test_split(malignant_df,test_size=0.2,random_state=0)\n",
    "print(train_benign.shape[0])\n",
    "print(val_benign.shape[0])\n",
    "print(train_malignant.shape[0])\n",
    "print(val_malignant.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26033\n",
      "6509\n",
      "26152\n",
      "6552\n"
     ]
    }
   ],
   "source": [
    "train_malignant_oversampled = train_malignant.copy()\n",
    "val_malignant_oversampled = val_malignant.copy()\n",
    "\n",
    "while(True):\n",
    "    if train_malignant_oversampled.shape[0] > train_benign.shape[0]:\n",
    "        break\n",
    "    train_malignant_oversampled = pd.concat([train_malignant_oversampled,train_malignant])\n",
    "\n",
    "while(True):\n",
    "    if val_malignant_oversampled.shape[0] > val_benign.shape[0]:\n",
    "        break\n",
    "    val_malignant_oversampled = pd.concat([val_malignant_oversampled,val_malignant])\n",
    "    \n",
    "print(train_benign.shape[0])\n",
    "print(val_benign.shape[0])\n",
    "print(train_malignant_oversampled.shape[0])\n",
    "print(val_malignant_oversampled.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52185, 2)\n",
      "(13061, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.concat([train_benign,train_malignant_oversampled])\n",
    "val_df = pd.concat([val_benign,val_malignant_oversampled])\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"target\"] = train_df[\"target\"].replace({0: 'benign', 1: 'malignant'}) \n",
    "val_df[\"target\"] = val_df[\"target\"].replace({0: 'benign', 1: 'malignant'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19144</th>\n",
       "      <td>ISIC_5809628.jpg</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11364</th>\n",
       "      <td>ISIC_3500137.jpg</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8017</th>\n",
       "      <td>ISIC_2497305.jpg</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>ISIC_1592874.jpg</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3291</th>\n",
       "      <td>ISIC_1092935.jpg</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_name  target\n",
       "19144  ISIC_5809628.jpg  benign\n",
       "11364  ISIC_3500137.jpg  benign\n",
       "8017   ISIC_2497305.jpg  benign\n",
       "4969   ISIC_1592874.jpg  benign\n",
       "3291   ISIC_1092935.jpg  benign"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.reset_index(drop=True)\n",
    "validate_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f00e253b0f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['target'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f00e1d6eda0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "validate_df['target'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total amount of data in train and validation set\n",
    "total_train = train_df.shape[0]\n",
    "total_validate = validate_df.shape[0]\n",
    "\n",
    "# set the minibatch size to 15\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=360,\n",
    "    rescale=1./255,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52185 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df, \n",
    "    \"../input/siim-isic-melanoma-classification/jpeg2/train/\", \n",
    "    x_col='image_name',\n",
    "    y_col='target',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=360,\n",
    "    rescale=1./255,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13061 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    validate_df, \n",
    "    \"../input/siim-isic-melanoma-classification/jpeg2/train/\",\n",
    "    x_col='image_name',\n",
    "    y_col='target',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 160, 160, 3)\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in train_generator:\n",
    "   break\n",
    "\n",
    "print(image_batch.shape)\n",
    "print(label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'benign', 1: 'malignant'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = dict((v,k) for k,v in train_generator.class_indices.items())\n",
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OkH-kazQecHB"
   },
   "source": [
    "## Create the base model from the pre-trained convnets\n",
    "You will create the base model from the **MobileNet V2** model developed at Google. This is pre-trained on the ImageNet dataset, a large dataset consisting of 1.4M images and 1000 classes. ImageNet is a research training dataset with a wide variety of categories like `jackfruit` and `syringe`. This base of knowledge will help us classify cats and dogs from our specific dataset.\n",
    "\n",
    "First, you need to pick which layer of MobileNet V2 you will use for feature extraction. The very last classification layer (on \"top\", as most diagrams of machine learning models go from bottom to top) is not very useful.  Instead, you will follow the common practice to depend on the very last layer before the flatten operation. This layer is called the \"bottleneck layer\". The bottleneck layer features retain more generality as compared to the final/top layer.\n",
    "\n",
    "First, instantiate a MobileNet V2 model pre-loaded with weights trained on ImageNet. By specifying the **include_top=False** argument, you load a network that doesn't include the classification layers at the top, which is ideal for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19IQ2gqneqmS"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 160\n",
    "\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "base_model = tf.keras.applications.ResNet152V2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AqcsxoJIEVXZ"
   },
   "source": [
    "This feature extractor converts each `160x160x3` image into a `5x5x1280` block of features. See what it does to the example batch of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-2LJL0EEUcx"
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[32,10,10,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:FusedBatchNormV3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-1f56a1b7acab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    715\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    716\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fused_batch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvirtual_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;31m# Currently never reaches here since fused_batch_norm does not support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36m_fused_batch_norm\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     output, mean, variance = tf_utils.smart_cond(\n\u001b[0;32m--> 553\u001b[0;31m         training, _fused_batch_norm_training, _fused_batch_norm_inference)\n\u001b[0m\u001b[1;32m    554\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bessels_correction_test_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m       \u001b[0;31m# Remove Bessel's correction to be consistent with non-fused batch norm.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m         pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[1;32m     58\u001b[0m   return smart_module.smart_cond(\n\u001b[0;32m---> 59\u001b[0;31m       pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/framework/smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36m_fused_batch_norm_inference\u001b[0;34m()\u001b[0m\n\u001b[1;32m    548\u001b[0m           \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m           \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m           data_format=self._data_format)\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     output, mean, variance = tf_utils.smart_cond(\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py\u001b[0m in \u001b[0;36mfused_batch_norm\u001b[0;34m(x, scale, offset, mean, variance, epsilon, data_format, is_training, name)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1501\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mfused_batch_norm_v3\u001b[0;34m(x, scale, offset, mean, variance, epsilon, data_format, is_training, name)\u001b[0m\n\u001b[1;32m   4231\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4232\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4233\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4234\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4235\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6605\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6606\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6607\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32,10,10,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:FusedBatchNormV3]"
     ]
    }
   ],
   "source": [
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rlx56nQtfe8Y"
   },
   "source": [
    "## Feature extraction\n",
    "In this step, you will freeze the convolutional base created from the previous step and to use as a feature extractor. Additionally, you add a classifier on top of it and train the top-level classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CnMLieHBCwil"
   },
   "source": [
    "### Freeze the convolutional base\n",
    "\n",
    "It is important to freeze the convolutional base before you compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. MobileNet V2 has many layers, so setting the entire model's trainable flag to False will freeze all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTCJH4bphOeo"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dLnpMF5KOALm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1024)\n"
     ]
    }
   ],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wv4afXKj6cVa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 2)\n"
     ]
    }
   ],
   "source": [
    "prediction_layer = tf.keras.layers.Dense(2, activation='softmax')\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0iqnBeZrfoIc"
   },
   "source": [
    "Now stack the feature extractor, and these two layers using a `tf.keras.Sequential` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eApvroIyn1K0"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "  prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g0ylJXE_kRLi"
   },
   "source": [
    "### Compile the model\n",
    "\n",
    "You must compile the model before training it.  Since there are two classes, use a binary cross-entropy loss with `from_logits=True` since the model provides a linear output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RpR8HdyMhukJ"
   },
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "krvBumovycVA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RxvgOYTDSWTx"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "After training for 10 epochs, you should see ~96% accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience=2, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class CustomSaver(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,directory):\n",
    "        super().__init__()\n",
    "        self.directory=directory\n",
    "        if(os.path.isfile(self.directory+'/history.csv')!=True):\n",
    "            history_df=pd.DataFrame(columns = ['loss','val_loss','accuracy','val_accuracy']) \n",
    "            history_df.to_csv(self.directory+'/history.csv',index=False)\n",
    "        self.history_df = pd.read_csv(self.directory+'/history.csv')\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print(\"\\nEpoch:\",epoch)\n",
    "        print(\"\\nLoss:\",logs[\"loss\"],\"Val_loss:\",logs[\"val_loss\"],\"Accuracy:\",logs[\"accuracy\"],\"Val_accuracy:\",logs[\"val_accuracy\"])\n",
    "        self.model.save_weights(self.directory+\"/model_\"+str(self.history_df.shape[0])+\".h5\")\n",
    "        print(\"\\nSaved weights: \"+self.directory+\"/model_\"+str(self.history_df.shape[0])+\".h5\")\n",
    "        self.history_df=self.history_df.append({'loss':logs[\"loss\"],\n",
    "                   'val_loss':logs['val_loss'],\n",
    "                   'accuracy':logs['accuracy'],\n",
    "                   'val_accuracy':logs['val_accuracy']},ignore_index = True)\n",
    "        self.history_df.to_csv(self.directory+'/history.csv',index=False)\n",
    "        \n",
    "    def latest_model(self):\n",
    "        if(os.path.isfile(self.directory+'/history.csv') and pd.read_csv(self.directory+'/history.csv').shape[0]>0):\n",
    "            iteration_to_be_loaded=pd.read_csv(self.directory+'/history.csv').shape[0]-1\n",
    "            print(self.directory+\"/model_\"+str(iteration_to_be_loaded)+\".h5\"+\" loaded!\")\n",
    "            return self.directory+\"/model_\"+str(iteration_to_be_loaded)+\".h5\"\n",
    "            #model.load_weights(\"saved_model_3/model_\"+str(iteration_to_be_loaded)+\".h5\")\n",
    "        else:\n",
    "            return self.directory+\"/model.h5\"\n",
    "    \n",
    "    def save_initial(self,model):\n",
    "        model.save_weights(self.directory+\"/model.h5\")\n",
    "         \n",
    "saver = CustomSaver(\"saved_model_5\")\n",
    "\n",
    "callbacks = [saver,learning_rate_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Om4O3EESkab1"
   },
   "outputs": [],
   "source": [
    "initial_epochs = 10\n",
    "validation_steps=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPXnzUK0QonF"
   },
   "source": [
    "### Un-freeze the top layers of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rfxv_ifotQak"
   },
   "source": [
    "All you need to do is unfreeze the `base_model` and set the bottom layers to be un-trainable. Then, you should recompile the model (necessary for these changes to take effect), and resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nzcagVitLQm"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4HgVAacRs5v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  87\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:]:\n",
    "  layer.trainable =  True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Uk1dgsxT0IS"
   },
   "source": [
    "### Compile the model\n",
    "\n",
    "Compile the model using a much lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtUnaz0WUDva"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=tf.optimizers.RMSprop(lr=base_learning_rate), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwBWy7J2kZvA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenet_1.00_160 (Model)   (None, 5, 5, 1024)        3228864   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 3,230,914\n",
      "Trainable params: 3,209,026\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bNXelbMQtonr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4G5O4jd6TuAG"
   },
   "source": [
    "### Continue training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0foWUN-yDLo_"
   },
   "source": [
    "If you trained to convergence earlier, this step will improve your accuracy by a few percentage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver.save_initial(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(saver.latest_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ECQLkAsFTlun"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1631 steps, validate for 409 steps\n",
      "Epoch 1/10\n",
      "1630/1631 [============================>.] - ETA: 0s - loss: 0.4704 - accuracy: 0.7765\n",
      "Epoch: 0\n",
      "\n",
      "Loss: 0.4702703369426764 Val_loss: 0.5326948606006091 Accuracy: 0.7765833 Val_accuracy: 0.74787533\n",
      "\n",
      "Saved weights: saved_model_5/model_0.h5\n",
      "1631/1631 [==============================] - 789s 484ms/step - loss: 0.4703 - accuracy: 0.7766 - val_loss: 0.5327 - val_accuracy: 0.7479\n",
      "Epoch 2/10\n",
      "1630/1631 [============================>.] - ETA: 0s - loss: 0.3428 - accuracy: 0.8499\n",
      "Epoch: 1\n",
      "\n",
      "Loss: 0.3428166495870788 Val_loss: 0.5061249107049555 Accuracy: 0.8498802 Val_accuracy: 0.77222264\n",
      "\n",
      "Saved weights: saved_model_5/model_1.h5\n",
      "1631/1631 [==============================] - 779s 478ms/step - loss: 0.3428 - accuracy: 0.8499 - val_loss: 0.5061 - val_accuracy: 0.7722\n",
      "Epoch 3/10\n",
      "1630/1631 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.8828\n",
      "Epoch: 2\n",
      "\n",
      "Loss: 0.2855090657158968 Val_loss: 0.5350882846379338 Accuracy: 0.88287824 Val_accuracy: 0.76632726\n",
      "\n",
      "Saved weights: saved_model_5/model_2.h5\n",
      "1631/1631 [==============================] - 782s 479ms/step - loss: 0.2855 - accuracy: 0.8829 - val_loss: 0.5351 - val_accuracy: 0.7663\n",
      "Epoch 4/10\n",
      "1630/1631 [============================>.] - ETA: 0s - loss: 0.2475 - accuracy: 0.9013\n",
      "Epoch: 3\n",
      "\n",
      "Loss: 0.2475166806249974 Val_loss: 0.6010350112081448 Accuracy: 0.901351 Val_accuracy: 0.74527216\n",
      "\n",
      "Saved weights: saved_model_5/model_3.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "1631/1631 [==============================] - 781s 479ms/step - loss: 0.2475 - accuracy: 0.9014 - val_loss: 0.6010 - val_accuracy: 0.7453\n",
      "Epoch 5/10\n",
      "1630/1631 [============================>.] - ETA: 0s - loss: 0.2155 - accuracy: 0.9181\n",
      "Epoch: 4\n",
      "\n",
      "Loss: 0.21554690610120908 Val_loss: 0.5467993140876439 Accuracy: 0.9180224 Val_accuracy: 0.7689304\n",
      "\n",
      "Saved weights: saved_model_5/model_4.h5\n",
      "1631/1631 [==============================] - 785s 481ms/step - loss: 0.2156 - accuracy: 0.9180 - val_loss: 0.5468 - val_accuracy: 0.7689\n",
      "Epoch 6/10\n",
      "1630/1631 [============================>.] - ETA: 0s - loss: 0.2025 - accuracy: 0.9229\n",
      "Epoch: 5\n",
      "\n",
      "Loss: 0.2024484857786909 Val_loss: 0.6358089033140821 Accuracy: 0.92292804 Val_accuracy: 0.74044865\n",
      "\n",
      "Saved weights: saved_model_5/model_5.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "1631/1631 [==============================] - 780s 478ms/step - loss: 0.2025 - accuracy: 0.9229 - val_loss: 0.6358 - val_accuracy: 0.7404\n",
      "Epoch 7/10\n",
      "1630/1631 [============================>.] - ETA: 0s - loss: 0.1910 - accuracy: 0.9299\n",
      "Epoch: 6\n",
      "\n",
      "Loss: 0.19106591879504845 Val_loss: 0.6472400091315248 Accuracy: 0.9299032 Val_accuracy: 0.73830485\n",
      "\n",
      "Saved weights: saved_model_5/model_6.h5\n",
      "1631/1631 [==============================] - 778s 477ms/step - loss: 0.1911 - accuracy: 0.9299 - val_loss: 0.6472 - val_accuracy: 0.7383\n",
      "Epoch 8/10\n",
      "1630/1631 [============================>.] - ETA: 0s - loss: 0.1827 - accuracy: 0.9323\n",
      "Epoch: 7\n",
      "\n",
      "Loss: 0.1827248684525821 Val_loss: 0.6614453333149912 Accuracy: 0.93233687 Val_accuracy: 0.7319501\n",
      "\n",
      "Saved weights: saved_model_5/model_7.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "1631/1631 [==============================] - 778s 477ms/step - loss: 0.1827 - accuracy: 0.9323 - val_loss: 0.6614 - val_accuracy: 0.7320\n",
      "Epoch 9/10\n",
      " 497/1631 [========>.....................] - ETA: 8:02 - loss: 0.1802 - accuracy: 0.9366\n",
      "Epoch: 8\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-3c537e6acbc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#                          initial_epoch =  history.epoch[-1],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                          callbacks = callbacks)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m                       total_epochs=1)\n\u001b[1;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m--> 397\u001b[0;31m                                  prefix='val_')\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    769\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/machine/lib/python3.6/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-3bb814e7a9ec>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpoch:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nLoss:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Val_loss:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Val_accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/model_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSaved weights: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/model_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "history_fine = model.fit(train_generator,\n",
    "                         epochs=10,\n",
    "#                          epochs=total_epochs,\n",
    "#                          initial_epoch =  history.epoch[-1],\n",
    "                         validation_data=validation_generator,\n",
    "                         callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TfXEmsxQf6eP"
   },
   "source": [
    "Let's take a look at the learning curves of the training and validation accuracy/loss when fine-tuning the last few layers of the MobileNet V2 base model and training the classifier on top of it. The validation loss is much higher than the training loss, so you may get some overfitting.\n",
    "\n",
    "You may also get some overfitting as the new training set is relatively small and similar to the original MobileNet V2 datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DNtfNZKlInGT"
   },
   "source": [
    "After fine tuning the model nearly reaches 98% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "chW103JUItdk"
   },
   "outputs": [],
   "source": [
    "history_df = pd.read_csv(\"saved_model_5/history.csv\")\n",
    "acc = history_df['accuracy']\n",
    "val_acc = history_df['val_accuracy']\n",
    "\n",
    "loss = history_df['loss']\n",
    "val_loss = history_df['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.470270</td>\n",
       "      <td>0.532695</td>\n",
       "      <td>0.776583</td>\n",
       "      <td>0.747875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.342817</td>\n",
       "      <td>0.506125</td>\n",
       "      <td>0.849880</td>\n",
       "      <td>0.772223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.285509</td>\n",
       "      <td>0.535088</td>\n",
       "      <td>0.882878</td>\n",
       "      <td>0.766327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.247517</td>\n",
       "      <td>0.601035</td>\n",
       "      <td>0.901351</td>\n",
       "      <td>0.745272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.215547</td>\n",
       "      <td>0.546799</td>\n",
       "      <td>0.918022</td>\n",
       "      <td>0.768930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.202448</td>\n",
       "      <td>0.635809</td>\n",
       "      <td>0.922928</td>\n",
       "      <td>0.740449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.191066</td>\n",
       "      <td>0.647240</td>\n",
       "      <td>0.929903</td>\n",
       "      <td>0.738305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.182725</td>\n",
       "      <td>0.661445</td>\n",
       "      <td>0.932337</td>\n",
       "      <td>0.731950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  val_loss  accuracy  val_accuracy\n",
       "0  0.470270  0.532695  0.776583      0.747875\n",
       "1  0.342817  0.506125  0.849880      0.772223\n",
       "2  0.285509  0.535088  0.882878      0.766327\n",
       "3  0.247517  0.601035  0.901351      0.745272\n",
       "4  0.215547  0.546799  0.918022      0.768930\n",
       "5  0.202448  0.635809  0.922928      0.740449\n",
       "6  0.191066  0.647240  0.929903      0.738305\n",
       "7  0.182725  0.661445  0.932337      0.731950"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"saved_model_5/history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_to_be_loaded = 1\n",
    "model.load_weights(\"saved_model_5/model_\"+str(iteration_to_be_loaded)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "20/20 [==============================] - 4s 203ms/step - loss: 0.4675 - accuracy: 0.7812\n",
      "initial loss: 0.47\n",
      "initial accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "loss0,accuracy0 = model.evaluate(validation_generator, steps = validation_steps)\n",
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10982"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_filenames = os.listdir(\"../input/siim-isic-melanoma-classification/jpeg2/test\")\n",
    "test_df = pd.DataFrame({\n",
    "    'filename': test_filenames\n",
    "})\n",
    "nb_samples = test_df.shape[0]\n",
    "nb_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10982 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#remember to not shuffle the test set\n",
    "test_generator = test_gen.flow_from_dataframe(\n",
    "    test_df, \n",
    "    \"../input/siim-isic-melanoma-classification/jpeg2/test/\", \n",
    "    x_col='filename',\n",
    "    y_col=None,\n",
    "    class_mode=None,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test_generator, steps=np.ceil(nb_samples/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10982, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.29205433, 0.7079457 ],\n",
       "       [0.99107224, 0.00892776],\n",
       "       [0.97972953, 0.02027045],\n",
       "       ...,\n",
       "       [0.90093464, 0.09906537],\n",
       "       [0.54657894, 0.4534211 ],\n",
       "       [0.7936603 , 0.20633972]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predict.shape)\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'benign', 1: 'malignant'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = dict((v,k) for k,v in train_generator.class_indices.items())\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7079457 , 0.00892776, 0.02027045, ..., 0.09906537, 0.4534211 ,\n",
       "       0.20633972], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['category']=predict[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_2951472.jpg</td>\n",
       "      <td>0.707946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0306094.jpg</td>\n",
       "      <td>0.008928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_9414809.jpg</td>\n",
       "      <td>0.020270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_1964724.jpg</td>\n",
       "      <td>0.315218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0547086.jpg</td>\n",
       "      <td>0.318643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10977</th>\n",
       "      <td>ISIC_1979700.jpg</td>\n",
       "      <td>0.015328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10978</th>\n",
       "      <td>ISIC_8328232.jpg</td>\n",
       "      <td>0.000268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10979</th>\n",
       "      <td>ISIC_9937768.jpg</td>\n",
       "      <td>0.099065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10980</th>\n",
       "      <td>ISIC_1608729.jpg</td>\n",
       "      <td>0.453421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10981</th>\n",
       "      <td>ISIC_3713800.jpg</td>\n",
       "      <td>0.206340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10982 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename  category\n",
       "0      ISIC_2951472.jpg  0.707946\n",
       "1      ISIC_0306094.jpg  0.008928\n",
       "2      ISIC_9414809.jpg  0.020270\n",
       "3      ISIC_1964724.jpg  0.315218\n",
       "4      ISIC_0547086.jpg  0.318643\n",
       "...                 ...       ...\n",
       "10977  ISIC_1979700.jpg  0.015328\n",
       "10978  ISIC_8328232.jpg  0.000268\n",
       "10979  ISIC_9937768.jpg  0.099065\n",
       "10980  ISIC_1608729.jpg  0.453421\n",
       "10981  ISIC_3713800.jpg  0.206340\n",
       "\n",
       "[10982 rows x 2 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = test_df.copy()\n",
    "# image_name,target\n",
    "submission_df['image_name'] = submission_df['filename'].str.split('.').str[0]\n",
    "submission_df['target'] = submission_df['category']\n",
    "submission_df.drop(['filename', 'category'], axis=1, inplace=True)\n",
    "# I added this line\n",
    "# submission_df=submission_df.astype({'id': 'int32'})\n",
    "# submission_df=submission_df.sort_values('id',ascending=True)\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_TZTwG7nhm0C"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "* **Using a pre-trained model for feature extraction**:  When working with a small dataset, it is a common practice to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is \"frozen\" and only the weights of the classifier get updated during training.\n",
    "In this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.\n",
    "\n",
    "* **Fine-tuning a pre-trained model**: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning.\n",
    "In this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the original dataset that the pre-trained model was trained on.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transfer_learning.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
