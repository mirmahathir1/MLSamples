{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "As we know,twitter tweets always have to be cleaned before we go onto modelling.So we will do some basic cleaning such as spelling correction,removing punctuations,removing html tags and emojis etc.So let's start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#Load the train and test set from the csv files\n",
    "tweet= pd.read_csv('../dataset/train.csv')\n",
    "test= pd.read_csv('../dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "   target  \n",
       "0     1.0  \n",
       "1     1.0  \n",
       "2     1.0  \n",
       "3     1.0  \n",
       "4     1.0  \n",
       "5     1.0  \n",
       "6     1.0  \n",
       "7     1.0  \n",
       "8     1.0  \n",
       "9     1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat the train and test dataframes\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\n",
    "df=pd.concat([tweet,test])\n",
    "\n",
    "#testing\n",
    "df.shape\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New competition launched :'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_URL(text):\n",
    "    # Compile a regular expression pattern into a regular expression object, \n",
    "    # which can be used for substitution later\n",
    "    \n",
    "    # '?' Causes the resulting RE to match 0 or 1 repetitions of the preceding RE. \n",
    "    # ab? will match either ‘a’ or ‘ab’.\n",
    "    \n",
    "    # '\\S' Matches any character which is not a whitespace character. \n",
    "    \n",
    "    # '+' Causes the resulting RE to match 1 or more repetitions of the preceding RE. \n",
    "    # ab+ will match ‘a’ followed by any non-zero number of ‘b’s; it will not match \n",
    "    # just ‘a’.\n",
    "\n",
    "    # A|B, where A and B can be arbitrary REs, creates a regular expression that \n",
    "    # will match either A or B\n",
    "    \n",
    "    # (Dot.) In the default mode, this matches any character except a newline\n",
    "    \n",
    "    # https://docs.python.org/3/library/re.html#re.compile\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "    # Return the string obtained by replacing the leftmost non-overlapping occurrences \n",
    "    # of pattern in string by the replacement ''\n",
    "    # https://docs.python.org/3/library/re.html#re.sub\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "#checking\n",
    "remove_URL(\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the remove_URL function on every text of df\n",
    "# keywords: apply lambda\n",
    "df['text']=df['text'].apply(lambda x : remove_URL(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReal or Fake\\nKaggle \\ngetting started\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_html(text):\n",
    "    # Compile a regular expression pattern into a regular expression object, \n",
    "    # which can be used for substitution later\n",
    "    \n",
    "    \n",
    "    # '*' Causes the resulting RE to match 0 or more repetitions of the preceding RE, \n",
    "    # as many repetitions as are possible. ab* will match ‘a’, ‘ab’, or ‘a’ \n",
    "    #followed by any number of ‘b’s.\n",
    "\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "#checking\n",
    "example = \"\"\"<div>\n",
    "<h1>Real or Fake</h1>\n",
    "<p>Kaggle </p>\n",
    "<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n",
    "</div>\"\"\"\n",
    "remove_html(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all html tags from all the texts of df\n",
    "df['text']=df['text'].apply(lambda x : remove_html(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Romoving Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Omg another Earthquake '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    # '[]' Used to indicate a set of characters.\n",
    "    # '\\u', '\\U', and '\\N' escape sequences are only recognized in Unicode patterns\n",
    "    # Ranges of characters can be indicated by giving two characters and separating them by a '-', \n",
    "    # for example [a-z] will match any lowercase ASCII letter, [0-5][0-9] will match all the two-digits numbers \n",
    "    # from 00 to 59, and [0-9A-Fa-f] will match any hexadecimal digit. If - is escaped (e.g. [a\\-z]) or \n",
    "    # if it’s placed as the first or last character (e.g. [-a] or [a-]), it will match a literal '-'.\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "#checking\n",
    "remove_emoji(\"Omg another Earthquake 😔😔\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a king\n"
     ]
    }
   ],
   "source": [
    "# a function to remove all punctuations\n",
    "def remove_punct(text):    \n",
    "    # create a translation table (dictionary which maps all '' to '' and all punctuation marks to None)\n",
    "    # get all the puncuations from the string module\n",
    "    # https://docs.python.org/3/library/stdtypes.html#str.maketrans\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    \n",
    "    # translate the text using the table\n",
    "    # https://docs.python.org/3/library/stdtypes.html#str.translate\n",
    "    return text.translate(table)\n",
    "\n",
    "#checking\n",
    "print(remove_punct(\"I am a #king\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation marks from the text column of df\n",
    "df['text']=df['text'].apply(lambda x : remove_punct(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if I'm not good at spelling I can correct it with python :) I will use `pyspellcheker` to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in /home/mahathir/miniconda3/envs/machine/lib/python3.6/site-packages (0.5.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'correct me please'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "# create a SpellChecker object\n",
    "# https://pyspellchecker.readthedocs.io/en/latest/code.html#spellchecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "def correct_spellings(text):\n",
    "    \n",
    "    # initialize a list to store the correct text\n",
    "    corrected_text = []\n",
    "    \n",
    "    # use the Specllcheker object to get the unknown words by splitting the text\n",
    "    # https://pyspellchecker.readthedocs.io/en/latest/code.html#spellchecker.SpellChecker.unknown\n",
    "    # keywords: split\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    \n",
    "    # check whether each word in the text is a correct word\n",
    "    # keywords: split\n",
    "    for word in text.split():\n",
    "        \n",
    "        # if the word is a misspelled word then correct the word and append it to corrected_text\n",
    "        # https://pyspellchecker.readthedocs.io/en/latest/code.html#spellchecker.SpellChecker.correction\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        # if the word is a correct word then append it to the corrected_text\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    \n",
    "    # join the list 'correct_text' with whitespaces and return\n",
    "    # https://docs.python.org/3/library/stdtypes.html#str.join\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "#checking\n",
    "correct_spellings(\"corect me plese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the 'text' column of df\n",
    "#df['text']=df['text'].apply(lambda x : correct_spellings(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe for Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 100 D here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10876/10876 [00:01<00:00, 7564.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10876\n",
      "[['our', 'deeds', 'reason', 'earthquake', 'may', 'allah', 'forgive', 'us'], ['forest', 'fire', 'near', 'la', 'ronge', 'sask', 'canada'], ['all', 'residents', 'asked', 'shelter', 'place', 'notified', 'officers', 'no', 'evacuation', 'shelter', 'place', 'orders', 'expected'], ['people', 'receive', 'wildfires', 'evacuation', 'orders', 'california'], ['just', 'got', 'sent', 'photo', 'ruby', 'alaska', 'smoke', 'wildfires', 'pours', 'school'], ['rockyfire', 'update', 'california', 'hwy', 'closed', 'directions', 'due', 'lake', 'county', 'fire', 'cafire', 'wildfires'], ['flood', 'disaster', 'heavy', 'rain', 'causes', 'flash', 'flooding', 'streets', 'manitou', 'colorado', 'springs', 'areas'], ['im', 'top', 'hill', 'i', 'see', 'fire', 'woods'], ['theres', 'emergency', 'evacuation', 'happening', 'building', 'across', 'street'], ['im', 'afraid', 'tornado', 'coming', 'area']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# function to create a corpus from a dataframe\n",
    "def create_corpus(df):\n",
    "    \n",
    "    # initialize corpus as an empty list\n",
    "    corpus=[]\n",
    "    \n",
    "    # Instantly make your loops show a smart progress meter - just wrap any iterable with tqdm(iterable)\n",
    "    # loop through the text column in df using tqdm\n",
    "    # https://tqdm.github.io/docs/tqdm/\n",
    "    for tweet in tqdm(df['text']):\n",
    "        \n",
    "        # tokenize each tweet. If all the characters in a word are letters and if the word is not a stopword\n",
    "        # then covert the word into lower case. Create an array of such processed words from a tweet\n",
    "        # https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize\n",
    "        # https://docs.python.org/3/library/stdtypes.html#str.isalpha\n",
    "        # keywords: for in if & not in\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
    "        \n",
    "        # append the processed words of a tweet to corpus\n",
    "        corpus.append(words)\n",
    "        \n",
    "    return corpus\n",
    "\n",
    "# create corpus from dataframe df\n",
    "corpus=create_corpus(df)\n",
    "\n",
    "#checking\n",
    "print(len(corpus))\n",
    "print(corpus[:10])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the',\n",
       "  array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "         -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "          0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "         -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "          0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "         -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "          0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "          0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "         -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "         -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "         -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "         -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "         -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "         -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "         -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "          0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "         -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)),\n",
       " (',',\n",
       "  array([-0.10767  ,  0.11053  ,  0.59812  , -0.54361  ,  0.67396  ,\n",
       "          0.10663  ,  0.038867 ,  0.35481  ,  0.06351  , -0.094189 ,\n",
       "          0.15786  , -0.81665  ,  0.14172  ,  0.21939  ,  0.58505  ,\n",
       "         -0.52158  ,  0.22783  , -0.16642  , -0.68228  ,  0.3587   ,\n",
       "          0.42568  ,  0.19021  ,  0.91963  ,  0.57555  ,  0.46185  ,\n",
       "          0.42363  , -0.095399 , -0.42749  , -0.16567  , -0.056842 ,\n",
       "         -0.29595  ,  0.26037  , -0.26606  , -0.070404 , -0.27662  ,\n",
       "          0.15821  ,  0.69825  ,  0.43081  ,  0.27952  , -0.45437  ,\n",
       "         -0.33801  , -0.58184  ,  0.22364  , -0.5778   , -0.26862  ,\n",
       "         -0.20425  ,  0.56394  , -0.58524  , -0.14365  , -0.64218  ,\n",
       "          0.0054697, -0.35248  ,  0.16162  ,  1.1796   , -0.47674  ,\n",
       "         -2.7553   , -0.1321   , -0.047729 ,  1.0655   ,  1.1034   ,\n",
       "         -0.2208   ,  0.18669  ,  0.13177  ,  0.15117  ,  0.7131   ,\n",
       "         -0.35215  ,  0.91348  ,  0.61783  ,  0.70992  ,  0.23955  ,\n",
       "         -0.14571  , -0.37859  , -0.045959 , -0.47368  ,  0.2385   ,\n",
       "          0.20536  , -0.18996  ,  0.32507  , -1.1112   , -0.36341  ,\n",
       "          0.98679  , -0.084776 , -0.54008  ,  0.11726  , -1.0194   ,\n",
       "         -0.24424  ,  0.12771  ,  0.013884 ,  0.080374 , -0.35414  ,\n",
       "          0.34951  , -0.7226   ,  0.37549  ,  0.4441   , -0.99059  ,\n",
       "          0.61214  , -0.35111  , -0.83155  ,  0.45293  ,  0.082577 ],\n",
       "        dtype=float32))]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize an empty dictionary so that we can add words as keys and the vector embedding of that word as value \n",
    "embedding_dict={}\n",
    "\n",
    "# Each line of glove.6B.100d.txt contains a word and it's glove vector embedding. Open the file in read mode\n",
    "# https://docs.python.org/3/library/functions.html#open\n",
    "with open('model/glove.6B.100d.txt','r') as f:\n",
    "    \n",
    "    #iterate through each line of the text\n",
    "    for line in f:\n",
    "        \n",
    "        #split each line by whitespaces\n",
    "        values=line.split()\n",
    "        \n",
    "        #get the 1st segment from the splitted line as word\n",
    "        word=values[0]\n",
    "        \n",
    "        # get the vector embeddings and turn it into a numpy array of float32\n",
    "        # https://numpy.org/devdocs/reference/generated/numpy.asarray.html\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        \n",
    "        # set the word and it's embedding as key value pair in the dictionary\n",
    "        embedding_dict[word]=vectors\n",
    "        \n",
    "# close the file\n",
    "# https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files\n",
    "f.close()\n",
    "\n",
    "#checking\n",
    "print(len(list(embedding_dict.items())))\n",
    "list(embedding_dict.items())[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd line of corpus as a list of words: ['all', 'residents', 'asked', 'shelter', 'place', 'notified', 'officers', 'no', 'evacuation', 'shelter', 'place', 'orders', 'expected']\n",
      "2nd line of corpus as a sequence of integers: [119, 1469, 1386, 2104, 645, 7972, 1667, 77, 204, 2104, 645, 1559, 1143]\n"
     ]
    }
   ],
   "source": [
    "# define max length of a sequence as 50\n",
    "MAX_LEN=50\n",
    "\n",
    "# create a Tokenizer object. This class allows to vectorize a text corpus, by turning each text into either \n",
    "# a sequence of integers (each integer being the index of a token in a dictionary).\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "tokenizer_obj=Tokenizer()\n",
    "\n",
    "# Update internal vocabulary of tokenizer based on a list of sequences.\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#fit_on_texts\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "\n",
    "# Transforms each word in corpus to a integer.\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "#checking\n",
    "print(\"2nd line of corpus as a list of words:\",corpus[2])\n",
    "print(\"2nd line of corpus as a sequence of integers:\",sequences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd line of corpus as a padded sequence: [ 119 1469 1386 2104  645 7972 1667   77  204 2104  645 1559 1143    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# Pads sequences to the same length (in our case, the MAX_LEN). \n",
    "# use truncating='post'. remove values from sequences larger than maxlen at the end of the sequences. \n",
    "# use padding='post'. pad after each sequence. \n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
    "\n",
    "#checking\n",
    "print(\"2nd line of corpus as a padded sequence:\",tweet_pad[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 20342\n",
      "[('i', 1), ('the', 2), ('like', 3), ('amp', 4), ('im', 5), ('a', 6), ('fire', 7), ('get', 8), ('new', 9), ('via', 10)]\n"
     ]
    }
   ],
   "source": [
    "# get the dictionary which contains every word's index in the tokenization process\n",
    "word_index=tokenizer_obj.word_index\n",
    "\n",
    "#checking\n",
    "print('Number of unique words:',len(word_index))\n",
    "print(list(word_index.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20342/20342 [00:00<00:00, 571497.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20343, 100)\n",
      "[[-0.046539    0.61966002  0.56647003 -0.46584001 -1.18900001  0.44599\n",
      "   0.066035    0.31909999  0.14679    -0.22119001  0.79238999  0.29905\n",
      "   0.16073     0.025324    0.18678001 -0.31000999 -0.28108001  0.60514998\n",
      "  -1.0654      0.52476001  0.064152    1.03579998 -0.40779001 -0.38011\n",
      "   0.30801001  0.59964001 -0.26991001 -0.76034999  0.94221997 -0.46919\n",
      "  -0.18278     0.90652001  0.79671001  0.24824999  0.25713     0.6232\n",
      "  -0.44768     0.65357     0.76902002 -0.51229    -0.44332999 -0.21867\n",
      "   0.38370001 -1.14830005 -0.94397998 -0.15062     0.30012    -0.57805997\n",
      "   0.20175    -1.65910006 -0.079195    0.026423    0.22051001  0.99713999\n",
      "  -0.57538998 -2.72659993  0.31448001  0.70521998  1.43809998  0.99125999\n",
      "   0.13976     1.34739995 -1.1753      0.0039503   1.02980006  0.064637\n",
      "   0.90886998  0.82871997 -0.47003001 -0.10575     0.5916     -0.42210001\n",
      "   0.57331002 -0.54114002  0.10768     0.39783999 -0.048744    0.064596\n",
      "  -0.61436999 -0.28600001  0.50669998 -0.49757999 -0.81569999  0.16407999\n",
      "  -1.96300006 -0.26693001 -0.37593001 -0.95846999 -0.85839999 -0.71577001\n",
      "  -0.32343    -0.43121001  0.41391999  0.28374001 -0.70931     0.15003\n",
      "  -0.2154     -0.37616    -0.032502    0.80620003]\n",
      " [-0.038194   -0.24487001  0.72812003 -0.39961001  0.083172    0.043953\n",
      "  -0.39140999  0.3344     -0.57545     0.087459    0.28786999 -0.06731\n",
      "   0.30906001 -0.26383999 -0.13231    -0.20757     0.33395001 -0.33848\n",
      "  -0.31742999 -0.48335999  0.1464     -0.37303999  0.34577     0.052041\n",
      "   0.44946    -0.46970999  0.02628    -0.54154998 -0.15518001 -0.14106999\n",
      "  -0.039722    0.28277001  0.14393     0.23464    -0.31020999  0.086173\n",
      "   0.20397     0.52623999  0.17163999 -0.082378   -0.71787    -0.41531\n",
      "   0.20334999 -0.12763     0.41367     0.55186999  0.57907999 -0.33476999\n",
      "  -0.36559001 -0.54856998 -0.062892    0.26583999  0.30204999  0.99774998\n",
      "  -0.80480999 -3.0243001   0.01254    -0.36941999  2.21670008  0.72201002\n",
      "  -0.24978     0.92136002  0.034514    0.46744999  1.10790002 -0.19358\n",
      "  -0.074575    0.23353    -0.052062   -0.22044     0.057162   -0.15806\n",
      "  -0.30798    -0.41624999  0.37972     0.15006    -0.53211999 -0.20550001\n",
      "  -1.25259995  0.071624    0.70564997  0.49744001 -0.42063001  0.26148\n",
      "  -1.53799999 -0.30223    -0.073438   -0.28312001  0.37103999 -0.25217\n",
      "   0.016215   -0.017099   -0.38984001  0.87423998 -0.72569001 -0.51058\n",
      "  -0.52028    -0.1459      0.82779998  0.27061999]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# add 1 to the number of unique words for an unknown token. \n",
    "# This token will have a vector embedding having only zeros\n",
    "num_words=len(word_index)+1\n",
    "\n",
    "# initialize a embedding matrix which will contain all the vector embeddings for our unique words of corpus.\n",
    "# https://numpy.org/devdocs/reference/generated/numpy.zeros.html\n",
    "embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "# get the word and index in each iteration of the dictionary items using tqdm\n",
    "# keywords: items\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    \n",
    "    # when the unknown word index appears, continue the loop\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    # get the embedding vector of the word from the embedding dictionary\n",
    "    # https://docs.python.org/3/library/stdtypes.html#dict.get\n",
    "    emb_vec=embedding_dict.get(word)\n",
    "    \n",
    "    if emb_vec is not None:#this None must not be changed\n",
    "        # Assign the embedding vector of that word into the 2D matrix\n",
    "        embedding_matrix[i]=emb_vec\n",
    "\n",
    "#checking\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a Sequential object. It groups a linear stack of layers into a tf.keras.Model.\n",
    "# https://keras.io/guides/sequential_model/\n",
    "model=Sequential()\n",
    "\n",
    "# add an embedding layer that Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "# input dimension size is the number of words,\n",
    "# output dimension size is 100,\n",
    "# set the embedding initializer to be an initializer that generates tensors with constant values. \n",
    "# Use embedding_matrix to feed the Constant initializer\n",
    "# https://keras.io/api/layers/core_layers/embedding/\n",
    "model.add(Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False))\n",
    "\n",
    "# The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, \n",
    "# which helps prevent overfitting.\n",
    "# SpatialDropout1D performs the same function as Dropout, \n",
    "# however it drops entire 1D feature maps instead of individual elements. Add a SpatialDropout1D layer where \n",
    "# Fraction of the input units to drop is 0.2 \n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/SpatialDropout1D\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "# Add an LSTM layer. dimensionality of the output space is 64, fraction of the units to drop for the linear \n",
    "# transformation of the inputs is 0.2 and Fraction of the units to drop for the linear transformation of the \n",
    "# recurrent state is 0.2\n",
    "# https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# add a Dense layer that implements the operation: output = activation(dot(input, kernel) + bias) where \n",
    "# activation is the element-wise activation function passed as the activation argument, \n",
    "# kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer \n",
    "# (only applicable if use_bias is True).\n",
    "# dimensionality of the output space is 1, Activation function to  should be sigmoid\n",
    "# https://keras.io/api/layers/core_layers/dense/\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# initialize Optimizer that implements the Adam algorithm. Adam optimization is a stochastic gradient descent \n",
    "# method that is based on adaptive estimation of first-order and second-order moments. \n",
    "# Learning rate should be 1e-5\n",
    "# https://keras.io/api/optimizers/adam/\n",
    "optimzer=Adam(learning_rate=1e-5)\n",
    "\n",
    "# Configure the model for training, set the optimizer instance,set the list of metrics to be evaluated by the \n",
    "# model during training and testing to 'accuracy' and name of objective function is binary_crossentropy\n",
    "# https://keras.io/api/models/model_training_apis/#compile-method\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 100)           2034300   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 2,076,605\n",
      "Trainable params: 42,305\n",
      "Non-trainable params: 2,034,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 50) (3263, 50)\n"
     ]
    }
   ],
   "source": [
    "# seperate the train and test set from tweet_pad\n",
    "train=tweet_pad[:tweet.shape[0]]\n",
    "test=tweet_pad[tweet.shape[0]:]\n",
    "\n",
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train (6471, 50)\n",
      "Shape of Validation  (1142, 50)\n"
     ]
    }
   ],
   "source": [
    "# split the train set into train and dev set. ratio should be 85:15\n",
    "X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\n",
    "print('Shape of train',X_train.shape)\n",
    "print(\"Shape of Validation \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6471 samples, validate on 1142 samples\n",
      "Epoch 1/15\n",
      " - 49s - loss: 0.6917 - accuracy: 0.5642 - val_loss: 0.6891 - val_accuracy: 0.5884\n",
      "Epoch 2/15\n",
      " - 48s - loss: 0.6850 - accuracy: 0.5681 - val_loss: 0.6625 - val_accuracy: 0.5884\n",
      "Epoch 3/15\n",
      " - 48s - loss: 0.6158 - accuracy: 0.6719 - val_loss: 0.5401 - val_accuracy: 0.7732\n",
      "Epoch 4/15\n",
      " - 48s - loss: 0.5755 - accuracy: 0.7283 - val_loss: 0.5172 - val_accuracy: 0.7706\n",
      "Epoch 5/15\n",
      " - 49s - loss: 0.5690 - accuracy: 0.7367 - val_loss: 0.5063 - val_accuracy: 0.7715\n",
      "Epoch 6/15\n",
      " - 48s - loss: 0.5589 - accuracy: 0.7418 - val_loss: 0.4999 - val_accuracy: 0.7785\n",
      "Epoch 7/15\n",
      " - 49s - loss: 0.5620 - accuracy: 0.7385 - val_loss: 0.4963 - val_accuracy: 0.7785\n",
      "Epoch 8/15\n",
      " - 49s - loss: 0.5528 - accuracy: 0.7427 - val_loss: 0.4909 - val_accuracy: 0.7820\n",
      "Epoch 9/15\n",
      " - 50s - loss: 0.5514 - accuracy: 0.7526 - val_loss: 0.4881 - val_accuracy: 0.7828\n",
      "Epoch 10/15\n",
      " - 50s - loss: 0.5426 - accuracy: 0.7543 - val_loss: 0.4868 - val_accuracy: 0.7846\n",
      "Epoch 11/15\n",
      " - 50s - loss: 0.5432 - accuracy: 0.7506 - val_loss: 0.4851 - val_accuracy: 0.7863\n",
      "Epoch 12/15\n",
      " - 50s - loss: 0.5488 - accuracy: 0.7450 - val_loss: 0.4850 - val_accuracy: 0.7881\n",
      "Epoch 13/15\n",
      " - 50s - loss: 0.5374 - accuracy: 0.7572 - val_loss: 0.4821 - val_accuracy: 0.7863\n",
      "Epoch 14/15\n",
      " - 51s - loss: 0.5427 - accuracy: 0.7580 - val_loss: 0.4817 - val_accuracy: 0.7881\n",
      "Epoch 15/15\n",
      " - 51s - loss: 0.5367 - accuracy: 0.7546 - val_loss: 0.4799 - val_accuracy: 0.7890\n"
     ]
    }
   ],
   "source": [
    "# fit the model with the train set. set batch size to be 4 and set number of epoch to be 15. set the dev set as\n",
    "# validation data and set verbose to be 2\n",
    "# https://keras.io/api/models/model_training_apis/#fit-method\n",
    "history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making our submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub=pd.read_csv('dataset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre=model.predict(test)\n",
    "y_pre=np.round(y_pre).astype(int).reshape(3263)\n",
    "sub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\n",
    "sub.to_csv('submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='5' color='red'>  if you like this kernel,please do an upvote.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
